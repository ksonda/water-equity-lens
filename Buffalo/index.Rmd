---
title: "Demonstration of Demographic Indexing for Water Equity Lens"
author: 
- Kyle Onda^[kyle.onda@ondawater.org]
date: "`r Sys.Date()`"
output:   
  html_document:
    toc: true
    toc_float: true
    toc_depth: 5
    code_folding: hide
    fig_width: 8
    fig_height: 8
    theme: readable
---

```{r setup, message="FALSE", include="FALSE"}
set.seed(12432)
library(tidyverse)
library(sf)
library(mapview)
library(tidycensus)
library(skimr)
library(knitr)
library(leafsync)
library(corrplot)
library(zoo)
library(missForest)
library(fuzzyjoin)
census_api_key("b25f8b1b7bf10561c9cbc3a20a4d2572677f1f05")
options(tigris_use_cache = TRUE)

#set certain global variables

state <- "NY" #state in which utility is
name <- "Buffalo" #name of service area
prj <- 4326 #relevant projection
alt_name <- "District" #name of alternative polygon

```

## Introduction

There is a large, diverse literature on creating indexes of what amounts to "Socioeconomic status" in spatial areas like neighborhoods. These indexes can then be used to investigate correlations between socioeconomics and health outcomes, infrastructure access, environmental exposures, or other variables, or target certain neighborhoods that seem particularly vulnerable for interventions. Many variables can be used to contribute to such an index. However, it is always difficult to choose which variables and how to weight them. For example, the USEPA EJSCREEN project tracks 6 demographic variables, but only uses two when constructing demographic indices to weight neighborhoods for an equity- weighting of environmental exposures: a simple average of % of population low-ioncome and % population "minority" (nonwhite alone/nonhispanic). One method that has been popular in social science and public health policy has been [Principal Components Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA). See [this study](https://academic.oup.com/heapol/article/21/6/459/612115) for a foundational example. PCA creates linear combinations of sets of variables that explain the maximum amount of overall variance in the complete dataset. This amounts to weighting the variables that have the widest spread and that covary with other variables. In this way, any arbitrary combination of variables of interest to policymakers/planners/analysts can be chosen, and a linear combination of them constructed for use as a single or multiple indices in an automated manner. This approach has the added value of being customizable for different communities, where different axes of inequity may be more or less important or impactful than in other communities. 

## Data sources

In the United States, typically this activity has been done to identify regions with socioeconomic correlations with cancer incidence. The typical source for variables has been the U.S Census American Community Survey, which includes some detailed demographic information about race/ethnicity, education levels, income, occupation, and housing quality, among other topics. 

Two or more of the following variables are often used:

* % of households living below 200% of the Federal Poverty Level 
* % of population of age >=25 with <= grade 12 education
* % of households nonwhite and nonhispanic (i.e. "% minority")
* Median household income
* per capita income
* % population on public assistance (SSI Disability, TANF, etc)
* median monthly rent
* median value of owner-occupied housing
* % population in owner-occupied housing
* % population with "blue collar jobs"
* Over 16 unemployment rate
* % Households with more than 1.5 person per room
* % Households without complete plumbing
* percent HH "linguistically isolated (no people speak english "very well" or better)
* % HH with no vehicle available.
* % single-parent families

Some studies using them are listed below:

* [Wheeler DC, Czarnota J, Jones RM (2017) Estimating an area-level socioeconomic status index and its association with colonoscopy screening adherence. PLOS ONE 12(6): e0179272.](https://doi.org/10.1371/journal.pone.0179272)

* [Yang J, Schupp CW, Harrati A, Clarke C, Keegan THM, Gomez SL. 
Developing an area-based socioeconomic measure from American Community Survey data.  
Cancer Prevention Institute of California, Fremont, California. 2014](https://cancerregistry.ucsf.edu/sites/g/files/tkssra1781/f/wysiwyg/Yang%20et%20al.%202014_CPIC_ACS_SES_Index_Documentation_3-10-2014.pdf)

* [Yu M, Tatalovich Z, Gibson JT, Cronin KA. Using a composite index of socioeconomic status to investigate health disparities while protecting the confidentiality of cancer registry data. Cancer Causes Control. 2014 Jan;25(1):81-92](https://pubmed.ncbi.nlm.nih.gov/24178398)

* [Yost, K., Perkins, C., Cohen, R. et al. Socioeconomic status and breast cancer incidence in California for different race/ethnic groups. Cancer Causes Control 12, 703â€“711 (2001)](https://doi.org/10.1023/A:1011240019516)

* [Kieger N. et al., Geocoding and Monitoring of US Socioeconomic Inequalities in Mortality and Cancer Incidence: Does the Choice of Area-based Measure and Geographic Level Matter? The Public Health Disparities Geocoding Project, American Journal of Epidemiology 156(5), 471-482 (2002)](https://doi.org/10.1093/aje/kwf068)

* [Robert Graham Center Social Deprivation Index](https://www.graham-center.org/rgc/maps-data-tools/sdi/social-deprivation-index.html)

## The workflow

First, we take an example service area boundary. In this case the Buffalo municipal boundary. This could come from an external API but in this case comes from a local file. We re-project to WGS84 just to be standard to begin with.

```{r, results="asis"}
#pws <- read_sf("C:/Users/kyleo/Downloads/pws.gpkg")
#newark <- filter(pws,PWSID=="NJ0714001")
boundary <- sf::read_sf("data/geospatial_data/COB_municipal_boundary.shp") %>% 
  st_transform(prj)
mapview(boundary)

```

Then we find the relevant Census Block Groups (orange) or Tracts (green) or ZCTA, first downloading the entire state and then subsetting to the utility service area (blue). We

```{r census_boundaries}
bg <- get_acs(state = state, geography = "block group", variables = c("B01001_001","B11016_001"), geometry = TRUE, output="wide") %>% st_transform(prj)
tr <- get_acs(state = state, geography = "tract", variables = c("B01001_001","B11016_001"), geometry = TRUE, output="wide") %>% st_transform(prj)
zip <- get_acs(state = state, geography = "zcta", variables = c("B01001_001","B11016_001"), geometry = TRUE, output="wide") %>% st_transform(prj)

bg1 <- st_intersects(bg,boundary) %>% as.integer()
bg <- bg[which(bg1==1),]

tr1 <- st_intersects(tr,boundary) %>% as.integer()
tr <- tr[which(tr1==1),]

zip1 <- st_intersects(zip,boundary) %>% as.integer()
zip <- zip[which(zip1==1),]

mapview(boundary, layer.name=name, map.types= "OpenStreetMap") +
  mapview(bg, alpha.regions=0, color="orange", col.regions="orange",lwd=2, layer.name="Block Groups", map.types= "OpenStreetMap") +
  mapview(zip, alpha.regions=0, color="green",col.regions="green", lwd=3, layer.name="ZCTA", map.types= "OpenStreetMap") 

```

Now we download the necessary census data and match them to the appropriate census boundaries. 

```{r census_data}
vars <- tidycensus::load_variables(2019, "acs5", cache = TRUE)
cv <- c(pop_race_count = "B03002_001",
        pop_nonhispanic_white_alone = "B03002_003",
                 pop_educ_attainment_count = "B15003_001",
                 pop_educ_none = "B15003_002", #0
                 pop_educ_nursery = "B15003_003", #0
                 pop_educ_kindergarten = "B15003_004", #0
                 pop_educ_grade1 = "B15003_005", #1
                 pop_educ_grade2 = "B15003_006", #2
                 pop_educ_grade3 = "B15003_007", #3
                 pop_educ_grade4 = "B15003_008", #4
                 pop_educ_grade5 = "B15003_009", #5
                 pop_educ_grade6 = "B15003_010", #6
                 pop_educ_grade7 = "B15003_011", #7
                 pop_educ_grade8 = "B15003_012", #8
                 pop_educ_grade9 = "B15003_013", #9
                 pop_educ_grade10 = "B15003_014", #10
                 pop_educ_grade11 = "B15003_015", #11
                 pop_educ_grade12_nodiploma = "B15003_016", #11.5
                 pop_educ_HSdiploma = "B15003_017", #12
                 pop_educ_GED = "B15003_018", #12
                 pop_educ_college_less_1year = "B15003_019", #13
                 pop_educ_college_more_1year_nodegree = "B15003_020", #13.5
                 pop_educ_assoc = "B15003_021", #14
                 pop_educ_bachelor = "B15003_022", #16
                 pop_educ_master = "B15003_023", #18
                 pop_educ_prof = "B15003_024", #19
                 pop_educ_doc = "B15003_025", # 21
                 poverty_level_total = "C17002_001",
                 poverty_level_gr_200FPL = "C17002_008" ,
                 hh_income_20ptile = "B19080_001",
                 hh_income_median = "B19013_001",
                 pop_hh_income_assistance_total = "B09010_001",
                 pop_in_hh_with_income_assistance = "B09010_002",
                 per_capita_income = "B19301_001",
                 pop_labor_force = "B23025_001",
                 pop_unemployed = "B23025_005",
                 housing_units_count = "B25002_001",
                 housing_units_vacant = "B25002_003",
                 housing_units_occupied = "B25002_002",
                 pop_units_count = "B25033_001",
                 pop_owner_occupied_units = "B25033_002",
                 pop_rental_units = "B25033_008",
                 housing_units_plumbing_count = "B25050_001",
                 housing_units_plumbing_complete = "B25050_002",
                 rent_median = "B25058_001",
                 median_gross_rent_percent_hh_income = "B25071_001",
                 home_value_median = "B25077_001",
                 hh_owner_occupied = "B25014_002",
                 hh_owner_occupied_1.5_2_perRoom = "B25014_006",
                 hh_owner_occupied_gr2.0_perRoom = "B25014_007",
                 hh_renters = "B25014_008",
                 hh_renters_1.5_2_perRoom = "B25014_012",
                 hh_renters_gr2.0_perRoom = "B25014_013",
                 hh_lang_total = "C16002_001",
                 hh_lang_ltd_api ="C16002_010",
                 hh_lang_ltd_otherio = "C16002_007",
                 hh_lang_ltd_spanish = "C16002_004",
                 hh_lang_ltd_other = "C16002_013",
                 pop_age_total = "B01001_001",
                 pop_age_male_under_5 = "B01001_003",
                 pop_age_female_under_5 = "B01001_027",
                 pop_age_male_65_66 = "B01001_020",
                 pop_age_male_67_69 = "B01001_021",
                 pop_age_male_70_74 = "B01001_022",
                 pop_age_male_75_79 = "B01001_023",
                 pop_age_male_80_84 = "B01001_024",
                 pop_age_male_85up = "B01001_025",
                 pop_age_female_65_66 = "B01001_044",
                 pop_age_female_67_69 = "B01001_045",
                 pop_age_female_70_74 = "B01001_046",
                 pop_age_female_75_79 = "B01001_047",
                 pop_age_female_80_84 = "B01001_048",
                 pop_age_female_85up = "B01001_049",
                 occupation_total = "C24010_001",
                 occupation_sales_office_male = "C24010_027",
                 occupation_sales_office_female = "C24010_063",
                 occupation_service_male = "C24010_019",
                 occupation_service_female = "C24010_055",
                 occupation_protective_service_male = "C24010_021",
                 occupation_protective_service_female = "C24010_057",
                 occupation_natresource_male = "C24010_030",
                 occupation_natresource_female = "C24010_066",
                 occupation_prodtrans_male = "C24010_034",
                 occupation_prodtrans_female = "C24010_070",
                hh_vehicle_count = "B25044_001",
                hh_owner_noVehicle = "B25044_003",
                hh_renter_noVehicle = "B25044_010",
                families_total = "B11003_001",
                families_singleparent_female = "B11003_016",
                families_singleparent_male = "B11003_010",
                hh_size_avg = "B25010_001"
        
        
    )


counties <- tidycensus::get_acs(year=2019,
                                geography = "county",
                                variables = "B01001_001",
                                geometry = TRUE)  %>% 
  sf::st_transform(prj) %>%
  dplyr::filter(lengths(sf::st_intersects(., boundary)) > 0)

# Find FIPS code for relevant state and counties, make sure no duplicates
st <- unique(substr(counties$GEOID,1,2)) 
ct <- unique(substr(counties$GEOID,3,5))

data.tr <- get_acs(year=2019,
                              geography = "tract",
                              variables = cv,
                              geometry = TRUE,
                              state = st,
                              county=ct,
                              output="wide") %>%
  filter(GEOID %in% tr$GEOID)

 data.bg <- get_acs(year=2019,
                              geography = "block group",
                              variables = cv,
                              geometry = TRUE,
                              state = st,
                              county=ct,
                              output="wide") %>%
  filter(GEOID %in% bg$GEOID)

  data.zip <- get_acs(year=2019,
                              geography = "zcta",
                              variables = cv,
                              geometry = TRUE,
                              state = st,
                              output="wide") %>%
  filter(GEOID %in% zip$GEOID)
```

Now we construct the variables for our PCA at the Tract and Block Group and Zip code level.

```{r variableConstruct}
pcaData.bg <- data.bg %>% 
  mutate(percBelow200FPL = 100*(1-poverty_level_gr_200FPLE/poverty_level_totalE),
         pop_total = pop_age_totalE,
         size_hh_avg = hh_size_avgE,
         percEducLessHS = 100*(1-(pop_educ_HSdiplomaE +
                        pop_educ_GEDE +
                        pop_educ_college_less_1yearE +
                        pop_educ_college_more_1year_nodegreeE + 
                        pop_educ_assocE +
                        pop_educ_masterE +
                        pop_educ_profE +
                        pop_educ_docE)/pop_educ_attainment_countE),
         percMinority = 100*(1-pop_nonhispanic_white_aloneE/pop_race_countE),
         income_median = hh_income_medianE,
         income_20tile = hh_income_20ptileE,
         income_percapita = per_capita_incomeE,
         percAssistance = 100*pop_in_hh_with_income_assistanceE /pop_hh_income_assistance_totalE,
         rentMedian = rent_medianE,
         rentIncomeRatioMedian = median_gross_rent_percent_hh_incomeE,
         homeValuemedian = home_value_medianE,
         percHomeowners = 100*pop_owner_occupied_unitsE/pop_units_countE,
         percBlueCollar = 100*(occupation_sales_office_maleE +
                                 occupation_sales_office_femaleE +
                                 occupation_service_maleE +
                                 occupation_service_femaleE -
                                 occupation_protective_service_maleE -
                                 occupation_protective_service_femaleE +
                                 occupation_natresource_maleE +
                                 occupation_natresource_femaleE +
                                 occupation_prodtrans_maleE +
                                 occupation_prodtrans_femaleE)/occupation_totalE,
         percUnemployed=pop_unemployedE/pop_labor_forceE,
         percPlumbingComplete = housing_units_plumbing_completeE/housing_units_plumbing_countE,
         percFamiliesSingleParent = 100*(families_singleparent_femaleE + families_singleparent_maleE)/families_totalE,
         percHH_NoVehicle = 100*(hh_owner_noVehicleE+hh_renter_noVehicleE)/hh_vehicle_countE,
         percAgeOver64 = 100*(pop_age_male_65_66E + 
                                pop_age_male_67_69E +
                                pop_age_male_70_74E +
                                pop_age_male_75_79E +
                                pop_age_male_80_84E +
                                pop_age_male_85upE +
                                pop_age_female_65_66E + 
                                pop_age_female_67_69E +
                                pop_age_female_70_74E +
                                pop_age_female_75_79E +
                                pop_age_female_80_84E +
                                pop_age_female_85upE)/pop_age_totalE,
         percAgeUnder5 = 100*(pop_age_male_under_5E + pop_age_female_under_5E)/pop_age_totalE,
         percHH_LangIsolated = 100 * (hh_lang_ltd_apiE +
                                        hh_lang_ltd_otherioE +
                                        hh_lang_ltd_spanishE +
                                        hh_lang_ltd_otherE)/hh_lang_totalE,
         percHH_Crowded = 100 * (hh_owner_occupied_1.5_2_perRoomE +
                                   hh_owner_occupied_gr2.0_perRoomE +
                                   hh_renters_1.5_2_perRoomE +
                                   hh_renters_gr2.0_perRoomE)/(hh_owner_occupiedE + hh_rentersE)) %>%
  select(GEOID,percBelow200FPL:percHH_Crowded) %>%
  st_drop_geometry()

pcaData.tr <- data.tr %>% 
  mutate(percBelow200FPL = 100*(1-poverty_level_gr_200FPLE/poverty_level_totalE),
         pop_total = pop_age_totalE,
         size_hh_avg = hh_size_avgE,
         percEducLessHS = 100*(1-(pop_educ_HSdiplomaE +
                        pop_educ_GEDE +
                        pop_educ_college_less_1yearE +
                        pop_educ_college_more_1year_nodegreeE + 
                        pop_educ_assocE +
                        pop_educ_masterE +
                        pop_educ_profE +
                        pop_educ_docE)/pop_educ_attainment_countE),
         percMinority = 100*(1-pop_nonhispanic_white_aloneE/pop_race_countE),
         income_median = hh_income_medianE,
         income_20tile = hh_income_20ptileE,
         income_percapita = per_capita_incomeE,
         percAssistance = 100*pop_in_hh_with_income_assistanceE /pop_hh_income_assistance_totalE,
         rentMedian = rent_medianE,
         rentIncomeRatioMedian = median_gross_rent_percent_hh_incomeE,
         homeValuemedian = home_value_medianE,
         percHomeowners = 100*pop_owner_occupied_unitsE/pop_units_countE,
         percBlueCollar = 100*(occupation_sales_office_maleE +
                                 occupation_sales_office_femaleE +
                                 occupation_service_maleE +
                                 occupation_service_femaleE -
                                 occupation_protective_service_maleE -
                                 occupation_protective_service_femaleE +
                                 occupation_natresource_maleE +
                                 occupation_natresource_femaleE +
                                 occupation_prodtrans_maleE +
                                 occupation_prodtrans_femaleE)/occupation_totalE,
         percUnemployed=pop_unemployedE/pop_labor_forceE,
         percPlumbingComplete = housing_units_plumbing_completeE/housing_units_plumbing_countE,
         percFamiliesSingleParent = 100*(families_singleparent_femaleE + families_singleparent_maleE)/families_totalE,
         percHH_NoVehicle = 100*(hh_owner_noVehicleE+hh_renter_noVehicleE)/hh_vehicle_countE,
         percAgeOver64 = 100*(pop_age_male_65_66E + 
                                pop_age_male_67_69E +
                                pop_age_male_70_74E +
                                pop_age_male_75_79E +
                                pop_age_male_80_84E +
                                pop_age_male_85upE +
                                pop_age_female_65_66E + 
                                pop_age_female_67_69E +
                                pop_age_female_70_74E +
                                pop_age_female_75_79E +
                                pop_age_female_80_84E +
                                pop_age_female_85upE)/pop_age_totalE,
         percAgeUnder5 = 100*(pop_age_male_under_5E + pop_age_female_under_5E)/pop_age_totalE,
         percHH_LangIsolated = 100 * (hh_lang_ltd_apiE +
                                        hh_lang_ltd_otherioE +
                                        hh_lang_ltd_spanishE +
                                        hh_lang_ltd_otherE)/hh_lang_totalE,
         percHH_Crowded = 100 * (hh_owner_occupied_1.5_2_perRoomE +
                                   hh_owner_occupied_gr2.0_perRoomE +
                                   hh_renters_1.5_2_perRoomE +
                                   hh_renters_gr2.0_perRoomE)/(hh_owner_occupiedE + hh_rentersE)) %>%
  select(GEOID,percBelow200FPL:percHH_Crowded) %>%
  st_drop_geometry()


pcaData.zip <- data.zip %>% 
  mutate(percBelow200FPL = 100*(1-poverty_level_gr_200FPLE/poverty_level_totalE),
         pop_total = pop_age_totalE,
         size_hh_avg = hh_size_avgE,
         percEducLessHS = 100*(1-(pop_educ_HSdiplomaE +
                        pop_educ_GEDE +
                        pop_educ_college_less_1yearE +
                        pop_educ_college_more_1year_nodegreeE + 
                        pop_educ_assocE +
                        pop_educ_masterE +
                        pop_educ_profE +
                        pop_educ_docE)/pop_educ_attainment_countE),
         percMinority = 100*(1-pop_nonhispanic_white_aloneE/pop_race_countE),
         income_median = hh_income_medianE,
         income_20tile = hh_income_20ptileE,
         income_percapita = per_capita_incomeE,
         percAssistance = 100*pop_in_hh_with_income_assistanceE /pop_hh_income_assistance_totalE,
         rentMedian = rent_medianE,
         rentIncomeRatioMedian = median_gross_rent_percent_hh_incomeE,
         homeValuemedian = home_value_medianE,
         percHomeowners = 100*pop_owner_occupied_unitsE/pop_units_countE,
         percBlueCollar = 100*(occupation_sales_office_maleE +
                                 occupation_sales_office_femaleE +
                                 occupation_service_maleE +
                                 occupation_service_femaleE -
                                 occupation_protective_service_maleE -
                                 occupation_protective_service_femaleE +
                                 occupation_natresource_maleE +
                                 occupation_natresource_femaleE +
                                 occupation_prodtrans_maleE +
                                 occupation_prodtrans_femaleE)/occupation_totalE,
         percUnemployed=pop_unemployedE/pop_labor_forceE,
         percPlumbingComplete = housing_units_plumbing_completeE/housing_units_plumbing_countE,
         percFamiliesSingleParent = 100*(families_singleparent_femaleE + families_singleparent_maleE)/families_totalE,
         percHH_NoVehicle = 100*(hh_owner_noVehicleE+hh_renter_noVehicleE)/hh_vehicle_countE,
         percAgeOver64 = 100*(pop_age_male_65_66E + 
                                pop_age_male_67_69E +
                                pop_age_male_70_74E +
                                pop_age_male_75_79E +
                                pop_age_male_80_84E +
                                pop_age_male_85upE +
                                pop_age_female_65_66E + 
                                pop_age_female_67_69E +
                                pop_age_female_70_74E +
                                pop_age_female_75_79E +
                                pop_age_female_80_84E +
                                pop_age_female_85upE)/pop_age_totalE,
         percAgeUnder5 = 100*(pop_age_male_under_5E + pop_age_female_under_5E)/pop_age_totalE,
         percHH_LangIsolated = 100 * (hh_lang_ltd_apiE +
                                        hh_lang_ltd_otherioE +
                                        hh_lang_ltd_spanishE +
                                        hh_lang_ltd_otherE)/hh_lang_totalE,
         percHH_Crowded = 100 * (hh_owner_occupied_1.5_2_perRoomE +
                                   hh_owner_occupied_gr2.0_perRoomE +
                                   hh_renters_1.5_2_perRoomE +
                                   hh_renters_gr2.0_perRoomE)/(hh_owner_occupiedE + hh_rentersE)) %>%
  select(GEOID,percBelow200FPL:percHH_Crowded) %>%
  st_drop_geometry()



```

Now, we inspect the distributions of the variables to see which ones may be problematic. It appears that the following variables are highly skewed and should be transformed:

* % Minority
* % houses with complete plumbing
* % Households in crowded positions (>= 1.5 people per room)

It also appears that the following variables are not available or otherwise highly missing at the block group level:

* % population on public assistance (100% missing)
* THe 20th percentile of HH income (100% missing)
* Median home value (11% missing)
* Median income (7% missing)

If clients are determined to use these variables, tract-level analysis may be necessary, which may not work for custom geographies that need to be assembled from smaller geographies than tracts.

We cannot use public assistance with Census Block Groups, but 20th percentile of income can be calculated from other means, and median home value can be imputed.

### Imputing missing data

We use a random forest missing value imputation algorithm (see [here](https://doi.org/10.1093/bioinformatics/btr597)) to fill in missing data for the purposes of later indexing. Compare median home values in the unimputed vs. imputed datasets

```{r impute, results="asis"}
imp.bg <- dplyr::select(pcaData.bg,-GEOID,-income_20tile,-percAssistance)
imp.bg <- missForest(imp.bg)
imp.bg <- imp.bg$ximp
imp.bg$GEOID <- pcaData.bg$GEOID

imp.tr <- dplyr::select(pcaData.tr,-GEOID)
imp.tr <- missForest(imp.tr)
imp.tr <- imp.tr$ximp
imp.tr$GEOID <- pcaData.tr$GEOID

imp.zip <- dplyr::select(pcaData.zip,-GEOID)
imp.zip <- missForest(imp.zip)
imp.zip <- imp.zip$ximp
imp.zip$GEOID <- pcaData.zip$GEOID


missing <- select(pcaData.bg,GEOID,homeValuemedian)
imputed <- select(imp.bg,GEOID,homeValuemedian)

imp.demo <- left_join(bg,missing,by="GEOID")
imp.demo <- left_join(imp.demo,imputed,by="GEOID")

map.miss <- mapview(imp.demo,zcol="homeValuemedian.x",layer.name="Median Home value (NA)")
map.imp <- mapview(imp.demo,zcol="homeValuemedian.y", layer.name="Median Home Value (imputed)")

sync(map.miss,map.imp)

```

### Alternative boundaries

With all missing data imputed, we can now demonstrate how U.S. Census data can be re-estimated for arbitrary larger geographies. Below, take the example of the City of Newark's [Neighborhood](https://data.ci.newark.nj.us/dataset/neighborhoods2013) boundaries, compared with Census Block Groups.

```{r otherbounds, results="asis"}
alt <- sf::read_sf("data/geospatial_data/COB_council_distrcits.shp") %>% st_transform(prj)

 mapview(bg, layer.name= "Census Block Groups") + mapview(alt, alpha.regions=0, col.regions="darkslategrey", lwd=3, color="darkslategrey", layer.name="Alt. Polygons (Neighborhoods)") + mapview(boundary, alpha.regions=0, lwd=2, color="black", col.regions="black",layer.name = name)

```
##### Reweighting for different polygons

In this case, Buffalo's alternative polygons seem to mostly be combinations of Census Block Groups, with minor discrepancies at borders. Other alternative polygons may be substantially different, so we implement a general weighted-area algorithm to reweight our census variables to the alternative polygon. The principle is the same as that documented [here](https://doi.org/10.1371/journal.pone.0245237.s003). 


First, we compute the area of the block groups (projecting to the appropriate state plane projection)
```{r weight}

bg <- bg %>% 
#  st_transform(prj) %>% # project to NAD83/New York West Projection
  mutate(area_bg = as.numeric(st_area(.)))# compute area
```

Then we compute the area of the alternative polygons (projecting to the appropriate state plane projection)

```{r weight2}
alt <- alt %>% 
#  st_transform(prj) %>% # project to NAD83/New York West Projection
  mutate(area_alt = as.numeric(st_area(.)))# compute area

```

Now we compute the spatial intersection of the alternate geometry with the block groups

```{r intersect}
alt.bg <- sf::st_intersection(alt,bg)

alt.bg <- alt.bg %>% 
  mutate(area_alt_bg = st_area(.), #calculate area of intersection of block group and alternative polygon
         share_area = area_alt_bg/area_alt, #calcualte share of area of each block group in each custom polygon
         pop = share_area * B01001_001E, #calculate population of each intersection
         hh = share_area * B11016_001E) %>% #calcualte hh of each intersection
  group_by(DIST_ID) %>% #group calculations by alternate polygon
  mutate(alt_pop = sum(pop), # calculate population of alternate polygons
         alt_hh = sum(hh) #calculate hh in alternate polygons
         ) %>% 
  ungroup() %>% #ungroup calculations
  mutate(wgt_pop = pop/alt_pop, #calculate prop of population of alternate polygon represented by each intersection
         wgt_hh = hh/alt_hh) #calculate prop of hh of alternate polygon represented by each intersection



```
Now we can weight every variable by hh and pop weights as appropriate to come up with the alternative polygon measures.

```{r transform}
imp.bg.long <- pivot_longer(imp.bg,!GEOID,names_to = "var",values_to = "value")
imp.bg.long <- left_join(imp.bg.long,alt.bg,by="GEOID")
imp.bg.long$value <- imp.bg.long$value * imp.bg.long$wgt_hh
imp.bg.long <- select(imp.bg.long,!!sym(alt_name),var,value)
imp.neigh.long <- imp.bg.long %>% 
  group_by(!!sym(alt_name),var) %>%  #change depending on alt polygon name
  summarise(value=sum(value))


alt_polygons <- alt %>% 
  left_join(imp.neigh.long,by=as.character(sym(alt_name))) %>% 
  select(!!sym(alt_name),var,value) %>%
  mutate(alt_polygon_id = !!sym(alt_name)) %>%
  select(alt_polygon_id,var,value) 
 
v <- as.data.frame(unique(alt_polygons$var))
v$units = c("USD",
            "USD",
            "USD",
            "%",
            "%",
            "%",
            "%",
            "%",
            "%",
            "%",
            "%",
            "%",
            "%",
            "%",
            "%",
            "%",
            "people",
            "ratio",
            "USD",
            "people"
            )

colnames(v) <- c("var","units")

alt_polygons <- na.omit(alt_polygons)
alt_data <- st_drop_geometry(alt_polygons)
alt_data <- left_join(alt_data,v,by="var")
alt_data$value <- as.numeric(alt_data$value)
alt_polygons <- distinct(select(alt_polygons,alt_polygon_id))
alt_polygons <- st_transform(alt_polygons,4326)

bg_data <- pivot_longer(imp.bg,!GEOID,names_to = "var",values_to = "value")
bg_data <- left_join(bg_data,v,by="var")
bg_polygons <- select(bg,GEOID)
boundary.p <- st_transform(boundary,prj) #change based on state plane
bg_polygons <- st_intersection(bg_polygons,boundary.p)
bg_polygons <- select(bg_polygons,GEOID)
bg_polygons <- st_transform(bg_polygons,4326)

zip_data <- pivot_longer(imp.zip,!GEOID,names_to = "var",values_to = "value")
zip_data <- left_join(zip_data,v,by="var")
zip_polygons <- select(zip,GEOID)
zip_polygons <- st_intersection(zip_polygons,boundary.p)
zip_polygons <- select(zip_polygons,GEOID)
zip_polygons <- st_transform(zip_polygons,4326)



bg_data$var <- paste0("census_",bg_data$var)
alt_data$var <- paste0("census_",alt_data$var)
zip_data$var <- paste0("census_",zip_data$var)

```



```{r inspect}
# skim(pcaData.tr)
# skim(pcaData.bg)
```
### PCA on census variables to create socioeconomic index

With this knowledge, we conduct the PCA, first on all the available variables for Block Groups and for Tracts. Below, we report on the first three principal components of the PCA for the tracts and block groups.

```{r transform2o, warning=FALSE, message=FALSE}
# is.nan.data.frame <- function(x)
# do.call(cbind, lapply(x, is.nan))
# 
# pcaData.bg[is.nan(pcaData.bg)]<-NA
# x=select(pcaData.bg,-GEOID,-percAssistance, -income_20tile)

bg_data_wide <- bg_data %>% pivot_wider(id_cols=GEOID,names_from = var, values_from = value)
pca.matrix.bg <- bg_data_wide %>% select(-GEOID)
pca.bg <- prcomp(pca.matrix.bg, scale=TRUE, center=TRUE, na.action=na.omit, rank=3)
bg_data_wide$PC1 <- pca.bg$x[,1]
bg_data <- pivot_longer(bg_data_wide,!GEOID,names_to = "var",values_to = "value")

zip_data_wide <- zip_data %>% pivot_wider(id_cols=GEOID,names_from = var, values_from = value)
pca.matrix.zip <- zip_data_wide %>% select(-GEOID)
pca.zip <- prcomp(pca.matrix.zip, scale=TRUE, center=TRUE, na.action=na.omit, rank=3)
zip_data_wide$PC1 <- pca.zip$x[,1]
zip_data <- pivot_longer(zip_data_wide,!GEOID,names_to = "var",values_to = "value")


alt_data_wide <- alt_data %>% distinct(alt_polygon_id,var,.keep_all=TRUE) %>% pivot_wider(id_cols=alt_polygon_id,names_from = var, values_from = value)



pca.matrix.alt <- alt_data_wide %>% select(-alt_polygon_id)
pca.alt <- prcomp(pca.matrix.alt, scale=TRUE, center=TRUE, na.action=na.omit, rank=3)
alt_data_wide$PC1 <- pca.alt$x[,1]
alt_data <- pivot_longer(alt_data_wide,!alt_polygon_id,names_to = "var",values_to = "value")
v$var <- paste0("census_",v$var)
alt_data <- left_join(alt_data,v,by="var")

colnames(alt_polygons)[1]<-"geo_id"
colnames(bg_polygons)[1]<-"geo_id"
colnames(zip_polygons)[1]<-"geo_id"

overall <- left_join(bg_data,select(bg,GEOID,B01001_001E),by="GEOID")
overall <- st_as_sf(overall)
overall$area_bg <- st_area(overall)
overall <- st_intersection(overall,select(boundary.p,OBJECTID))
overall$area_intersection <- st_area(overall)
overall$area_share <- overall$area_intersection/overall$area_bg
overall$wgt <- (overall$area_share * overall$B01001_001E) / sum(bg$B01001_001E)
overall$value <- overall$value * overall$wgt

overall <- overall %>% st_drop_geometry() %>% group_by(var) %>% summarise(value=sum(value))

n <- boundary %>% select(OBJECTID)
colnames(n)[1]<-"geo_id"

n$geo_type <- "Service Area"
alt_polygons$geo_type <- alt_name
bg_polygons$geo_type <- "Census Block Group"
zip_polygons$geo_type <- "Census Zip Code Tabulation Area"


write_sf(n,"finaldata/service_area.geojson",delete_dsn=TRUE)
write_sf(alt_polygons,"finalData/alternate.geojson",delete_dsn=TRUE)
write_sf(bg_polygons,"finaldata/block_group.geojson",delete_dsn=TRUE)
write_sf(zip_polygons,"finaldata/zcta.geojson",delete_dsn=TRUE)

pca_summary1 <- pca.bg$rotation %>% as.data.frame() %>% arrange(desc(abs(PC1)))
pca_summary2 <- pca.bg$rotation %>% as.data.frame() %>% arrange(desc(abs(PC2)))
```

### Creating Indicator Data model.

First, we create a vector of the indicator variables that are in the [data dictionary](data/WEL data dictionary_r2.xlsx), and then create an empty data frame encompassing all polygons and these variables, iterated over years of choice. 

```{r}
year <- c(2010,2020)
var <- c("pillar1_hbi",
         "pillar1_ppi",
         "pillar1_affordability",
         "pillar1_cutoff",
         "pillar1_cap",
         "pillar1_connections_water",
         "pillar1_connections_sewer",
         "pillar1_complaints",
         "pillar2_workforce",
         "pillar2_diversity",
         "pillar2_procurement",
         "pillar2_training",
         "pillar2_park_distance",
         "pillar2_park_walk_access",
         "pillar2_waterfront_acess",
         "pillar3_breaks",
         "pillar3_sewer_failures",
         "pillar3_lead_lines",
         "pillar3_lead_line_replacement",
         "pillar3_proactive_maintenance",
         "pillar3_outreach",
         "pillar3_meetings",
         "pillar3_water_board",
         "pillar3_flooding"
         )

geo_type <- c("Census Block Group",alt_name,"Census Zip Code Tabulation Area","Service Area")

geo_id <- unique(bg_data$GEOID)
geo_id <- as.data.frame(c(geo_id,unique(alt_data$alt_polygon_id),unique(zip_data$GEOID)))
geo_id <- rbind(geo_id,name)

colnames(geo_id)[1] <- "geo_id"

pillars <- expand_grid(geo_id,year,var) 

l_id <- length(geo_id[,1])
l_bg <- length(unique(bg_data$GEOID))
l_zip <- length(unique(zip_data$GEOID))
l_alt <- length(unique(alt_data$alt_polygon_id))
l_vars <- length(var)
l_year <- length(year)

l_bg <- l_bg*l_vars*l_year
l_alt <- l_alt*l_vars*l_year
l_zip <- l_zip*l_vars*l_year

pillars$geo_type <- "Census Block Group"
pillars$geo_type[(l_bg+1):(l_bg+l_alt)] <- alt_name
pillars$geo_type[(l_bg+l_alt+1):(l_bg+l_alt+l_zip)] <- "Census Zip Code Tabulation Area"
pillars$geo_type[(l_bg+l_alt+l_zip+1):length(pillars$geo_type)] <- "Service Area"

pillars$units <- "%"
pillars$units[which(pillars$var %in% c("pillar2_diversity",
                                       "pillar2_training",
                                       "pillar3_meetings",
                                       "pillar2_workforce",
                                       "pillar2_public_facing",
                                       "pillar3_water_board",
                                       "pillar3_breaks",
                                       "pillar3_sewer_failures"))] <- "ratio"
```

Below, we populate the indicator data model with random uniform variables. This step should be replaced by an entire workflow that impots submitted data and calculates the variables
```{r}

pillars <- pillars %>% group_by(var,year,geo_id) %>% mutate(value=runif(1))
pillars$value[which(pillars$units=="%")] <- 100*pillars$value[which(pillars$units=="%")]

```

Below, we calculate the percent change year-over-year in the indicators for each polygon, where applicable

```{r}

pillars <- pillars %>% 
  arrange(geo_id,var,year) %>% 
  group_by(geo_id,var) %>% 
  mutate(delta_percent = 100*(value - lag(value))/lag(value)) %>% 
  ungroup()

pillars <- select(pillars,geo_id,geo_type,year,var,value,units,delta_percent)

pillars$units[which(pillars$var=="pillar1_affordability")] <- "character"
pillars$value[which(pillars$var=="pillar1_affordability")] <- sample(c("Low","Medium","High"),length(pillars$var[which(pillars$var=="pillar1_affordability")]),replace=TRUE)
pillars$delta_percent[which(pillars$units=="character")]<-NA 
write_csv(pillars,"finalData/indicators.csv")
```

<!-- Here, we demonstrate how a "bucket" can be specified a priori for each variable, and then used to calculate the appropriate bucket in the data. Here we use the `fuzzyjoin` R package, but the equivalent logic is in this SQL query  -->
<!-- ```{r, eval=FALSE} -->
<!-- 'SELECT vars, value, bucket_names  -->
<!--       FROM pillars  -->
<!--       LEFT JOIN buckets ON value BETWEEN lower_bound and upper_bound' -->

<!-- ``` -->

<!-- ```{r} -->
<!-- buckets <- read_csv("data/buckets.csv") %>% select(vars,bucket_names,lower_bound,upper_bound) -->
<!-- pillars$val_numeric <- as.numeric(pillars$value) -->

<!-- library(fuzzyjoin) -->
<!-- # Attempt #3: use the fuzzyjoin package -->
<!-- pillars<-fuzzy_left_join(pillars, buckets,  -->
<!--                 by=c("val_numeric"="lower_bound", "val_numeric"="upper_bound"), -->
<!--                 match_fun=list(`>=`, `<=`)) %>%  -->
<!--   select(geo_id:units,bucket_names,delta_percent) %>% -->
<!--   rename(var_bucket=bucket_names)%>% -->
<!--   distinct(.keep_all=TRUE) -->

<!-- ``` -->

<!-- Below, we calculate an indicator variable for which quartile a given polygon's percent change (delta_percent) is.  -->

<!-- ```{r} -->
<!-- qe <- pillars %>%  -->
<!--   group_by(geo_type,var,year) %>% -->
<!--   summarise(delta_quartile=quantile(delta_percent, -->
<!--                                     probs=c(0.25,0.5,0.75),na.rm=TRUE),  -->
<!--             q=c("p25","p50","p75")) %>% -->
<!--   pivot_wider(names_from=q, values_from=delta_quartile) -->

<!-- qe <- left_join(pillars,qe,by=c("geo_type","var","year")) -->
<!-- qe$delta_quartile = NA -->
<!-- qe <- qe %>% mutate( -->
<!--   delta_quartile = case_when(delta_percent <= p25 ~ 1, -->
<!--                              delta_percent > p25 & delta_percent <= p50 ~ 2, -->
<!--                              delta_percent > p50 & delta_percent <= p75 ~ 3, -->
<!--                              delta_percent > p75 ~ 4) -->
<!-- ) -->
<!-- ``` -->

<!-- Below, we isolate only the columns of interest and export the indicator data to csv.  -->
<!-- ```{r} -->

<!-- pillars <- qe %>% select(geo_id,geo_type,year,var,units,value,var_bucket,delta_percent,delta_quartile) -->

<!-- write_csv(pillars,"finalData/indicators.csv") -->
<!-- ``` -->

Below, we add the block group and alternate polygon and overall aggregated census data into one long-form data frame 

```{r}
bg_data2 <- left_join(bg_data,v,by="var")
zip_data2 <- left_join(zip_data,v,by="var")

alt_data$geo_type <- alt_name
alt_data$geo_id <- alt_data$alt_polygon_id
alt_data <- select(alt_data,geo_id,geo_type,var,value,units)

bg_data2$geo_type <- "Census Block Group"
bg_data2$geo_id <- bg_data2$GEOID
bg_data2 <- select(bg_data2,geo_id,geo_type,var,value,units)

zip_data2$geo_type <- "Census Zip Code Tabulation Area"
zip_data2$geo_id <- zip_data$GEOID
zip_data2 <- select(zip_data2,geo_id,geo_type,var,value,units)

overall$geo_id <- name
overall$geo_type <- "Service Area"
overall$value <- as.numeric(overall$value)
overall2 <- left_join(overall,v,by="var")
overall2 <- select(overall2,geo_id,geo_type,var,value,units)

census <- bind_rows(bg_data2,alt_data,zip_data2,overall2)
census$year <- 2019
census <- select(census,geo_id,geo_type,year,var,value,units)
census2 <- census
census2$year <- NA
census2$value <- NA
census <- bind_rows(census,census2) 
census <- census %>% arrange(geo_type,geo_id,var,year)
census$delta_percent <- NA
write_csv(census,"finalData/census.csv") 
```

<!-- Below, we add quartiles for each census variable. -->

<!-- ```{r} -->

<!-- qc <-census %>%  -->
<!--   group_by(geo_type,var,year) %>% -->
<!--   summarise(var_quartile=quantile(value, -->
<!--                                     probs=c(0.25,0.5,0.75),na.rm=TRUE),  -->
<!--             q=c("p25","p50","p75")) %>% -->
<!--   pivot_wider(names_from=q, values_from=var_quartile) -->

<!-- qc <- left_join(census,qc,by=c("geo_type","var","year")) -->
<!-- qc$var_quartile = NA -->
<!-- qc <- qc %>% mutate( -->
<!--   var_quartile = case_when(value <= p25 ~ 1, -->
<!--                              value > p25 & value <= p50 ~ 2, -->
<!--                              value > p50 & value <= p75 ~ 3, -->
<!--                              value > p75 ~ 4) -->
<!-- ) -->

<!-- ``` -->
<!-- Below, we isolate only the columns of interest and export the indicator data to csv.  -->
<!-- ```{r} -->

<!-- census <- qc %>% select(geo_id,geo_type,year,var,units,value,var_quartile) -->

<!-- write_csv(census,"finalData/census.csv") -->
<!-- ``` -->


### Insights: most correlated variables

There are many sophisticated methods from the "machine learning" toolbox (e.g. stepwise regression algorithms, ridge regressions, LASSO, random forest models, ) that could serve to identify particular models based on sociodemographic/economic variables that most effectively predict values of equity/performance variables. However, it may be more straightforward to justify and communicate highlighting which sociodemographic variables are most linearly correlated with a variable of interest.



```{r}

x = census %>% filter(geo_type=="block_group" & year==2019) %>% pivot_wider(id_cols=geo_id,names_from = var, values_from = value) %>% arrange(geo_id) %>% select(-geo_id)

y = pillars %>% mutate(value=runif(n=length(pillars$geo_type))) %>% filter(geo_type=="block_group" & year == 2020)  %>% pivot_wider(id_cols=geo_id,names_from = var, values_from = value) %>% arrange(geo_id) %>% select(-geo_id)

c <- cor(x=x,y=y)
c <- as.data.frame(c)
c$census_var <- rownames(c)
c <- pivot_longer(c,cols=pillar1_affordability:pillar3_water_board ,names_to = "variable", values_to="corr")

write_csv(c,"finalData/correlations.csv")

```


## Results 

For Districts, we see that the first component alone explains 45% of the variance in the specified variables. Higher values on this component are associated with lower income measures, higher poverty, public assistance and unemployment, lower rents and housing values, more blue collar employment, higher proportions of minority groups, lower homeownership rates, less vehicle ownership, and higher prevalence of single-parent families, without being particularly informed by linguistic isolation, education levels, or housing crowdedness.

```{r, results="asis"}
kable(summary(pca.alt)$importance[1:3,1:3],digits=2)
kable(pca.alt$rotation,digits=2)
```

For block groups, we see that the first component alone explains only 38% of the variance in the specified variables (compared to 45% in the District-level analysis). However, the way the variables contribute to the first component is similar to how they do in the tract-level analysis.

```{r, results="asis"}
kable(summary(pca.bg)$importance[1:3,1:3],digits=2)
kable(pca.bg$rotation,digits=2)
```

Below, we visualize how the first component (PC1) can be used as a multidimensional socioeconomic status/ deprivation index, that incorporates more information and yields different results than just using income or poverty levels or minority group prevalence. 

```{r, results="asis"}
bgm <- left_join(bg,bg_data_wide,by="GEOID")
# tr <- left_join(tr,pcaData.tr,by="GEOID")
# tr$PC1_allVars <- -tr$PC1_allVars

m.pc.bg <- mapview(bgm,zcol="PC1", layer.name="Block Groups \n PC1") + mapview(boundary,alpha.regions=0,col.regions="red",color="red",layer.name="BSA.", lwd=2)
m.pov.bg <- mapview(bgm,zcol="census_income_median", layer.name="Block Groups \n Median HH Income") + mapview(boundary,alpha.regions=0,col.regions="red",color="red",layer.name="BSA", lwd=2)
# 
# m.pc.tr <- mapview(tr,zcol="PC1_allVars", layer.name="Tracts \n PC1") + mapview(newark,alpha.regions=0,col.regions="red",color="red",layer.name="BSA", lwd=2)
# m.pov.tr <- mapview(tr,zcol="income_median", layer.name="Tracts \n Median HH Income") + mapview(newark,alpha.regions=0,col.regions="red",color="red",layer.name="BSA", lwd=2)
# 
 mapl <- sync(m.pc.bg, m.pov.bg)
mapl


```

<!-- ## Using indexes and correlation indexes to create insights. -->

<!-- ### Scaling variables by indexes. -->

<!-- Below we demonstrate how using an index can be used to highlight neighborhoods that users may be more interested in from an equity framework. For example, if the distribution of a problematic phenomenon such as taste complaints is randomly distributed, then from an equity perspective, one may wish to prioritize addressing these issues in the most socioeconomically burdened neighborhoods before less burdened neighborhoods. -->

<!-- We simulate tract-level taste complaints per household as a normally distributed random variable with a mean of 0.2 and a standard deviation of 0.1 and a minimum of 0, with no assumed correlation with any equity-related variable: -->

<!-- ```{r taste, results="asis"} -->
<!-- set.seed(2354367) -->
<!-- tr$taste_complaints_perHH <- rnorm(n=130,mean=0.2, sd=0.1) -->
<!-- tr$taste_complaints_perHH[which(tr$taste_complaints_perHH<0)] <- 0 -->
<!-- tr$PC1_allVars <- -tr$PC1_allVars -->

<!-- map.tr <- mapview::mapview(tr,zcol="taste_complaints_perHH", layer.name="Tracts \n Taste Compl per HH.") -->

<!-- map.pc <- mapview::mapview(tr,zcol="PC1_allVars", layer.name="Tracts \n Burden Index") -->

<!-- sync(map.pc, map.tr) -->


<!-- ggplot2::ggplot(data=tr, aes(x=-PC1_allVars,y=taste_complaints_perHH)) + geom_point() + geom_abline() + xlab("Equity Principal Component (higher = more burdened)") -->
<!-- ``` -->

<!-- As we can see above, there is no correlation between our simulated taste complaint indicator and the socioeconomic status index (scale reversed, so that higher numbers correspond to a more "burdened" area). -->

<!-- We can multiply the taste complaint indicator by the "burden" index to create a taste complaint scaled by burden that will highlight different areas to focus on than by focusing on the indicator alone: -->

<!-- ```{r scale demo, results="asis"} -->
<!-- tr$taste_scaled <- tr$PC1_allVars* tr$taste_complaints_perHH -->

<!-- map.tr <- mapview::mapview(tr,zcol="taste_complaints_perHH", layer.name="Tracts \n Taste Compl per HH.") -->

<!-- map.pc <- mapview::mapview(tr,zcol="taste_scaled", layer.name="Taste complaints scaled by burden") -->

<!-- sync(map.pc, map.tr) -->

<!-- ``` -->

<!-- ### Insights: most correlated variables -->

<!-- There are many sophisticated methods from the "machine learning" toolbox (e.g. stepwise regression algorithms, ridge regressions, LASSO, random forest models, ) that could serve to identify particular models based on sociodemographic/economic variables that most effectively predict values of equity/performance variables. However, it may be more straightforward to justify and communicate highlighting which sociodemographic variables are most linearly correlated with a variable of interest. -->

<!-- Let's simulate some equity indicators that are various functions of the socioeconomic variables: -->

<!-- ```{r y_simuolate} -->
<!-- x <- tr[6:31] %>%  -->
<!--   st_drop_geometry() %>%  -->
<!--   na.aggregate() -->

<!-- pca <- pca.tr$x -->

<!-- y1 <- x$PC1 + rnorm(length(x$PC1),0,1) -->
<!-- y2 <- 0.5*x$PC1 + 0.5*x$PC2 + rnorm(length(x$PC1),0,1) -->
<!-- y3 <- 0.5*x$PC3 + 0.5*x$PC2 + rnorm(length(x$PC1),0,1) -->
<!-- ``` -->

<!-- Now we compute the correlation between each of these simulated indicators and all of the socioeconomic variables. Correlation is measured on a scale between -1 and 1, with values closer to 0 indicating less correlation. The table below shows how each of the three simulated equity indicators are correlated with each of the socioeconomic variables. For example, $y1$ is highly correlated with the percentage of the households below 200% of the Federal Poverty level, Single parent families, and households with no vehicle,  and strongly inversely correlated with income measures, home values, and home ownership rates. $y3$, on the other hand, is most highly correlated with lower levels of linguistic isolation and education levels below high school. This would enable a dynamic insight to be generated, where a user could select an equity indicator, and be shown any relevant descriptions or plots for any associated socioeconomic variables.  -->

<!-- ```{r corr, results="asis"} -->
<!-- y <- as.data.frame(cbind(y1,y2,y3)) -->
<!-- x <- tr[6:24] %>%  -->
<!--   st_drop_geometry() %>%  -->
<!--   na.aggregate() -->
<!-- c <- cor(x=x,y=y) -->

<!-- test <- cor.mtest(cbind(y,x))$p[1:3,4:22] -->

<!-- corrplot(c, -->
<!--   method = "number", -->
<!--   type = "full", -->
<!--   order="original"# show only upper side -->
<!-- ) -->
<!-- ``` -->
<!-- Saving our data at the tract and block group level -->
<!-- ```{r save} -->
<!-- sf::write_sf(bg,"finalData/census_blockgroup_buffalo_withdata.geojson") -->
<!-- sf::write_sf(tr,"finalData/census_tract_buffalo_withdata.geojson") -->
<!-- sf::write_sf(buffalo,"finalData/service_area_buffalo_withdata.geojson") -->
<!-- ``` -->
